import random
import math

'''
This agent is updated each time a step is made to keep track of the last state and action selected by the machine. The states are created

'''

class LearningAgent():
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """

    def __init__(self, learning=False, epsilon=1.0, alpha=0.5):

        # Set parameters of the learning agent
        self.learning = learning                        # Whether the agent is expected to learn
        self.Q = dict()                                 # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon                          # Random exploration factor
        self.alpha = alpha                              # Learning factor
        self.valid_actions = [i for i in range(9)]      # valid actions corresponding to the indices on the array
        self.last_state = None                          # last index selected
        self.last_action = None                         # last index selected

        ###########
        ## TO DO ##
        ###########
        # Set any additional class parameters as needed

        # This parameter will count the number of times the reset function is called, which would equate to the number of trials

        self.trial_count = 0.0

    def reset(self, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        # self.planner.route_to(destination)

        ###########
        ## TO DO ##
        ###########
        # Update epsilon using a decay function of your choice
        # Update additional class parameters as needed
        # If 'testing' is True, set epsilon and alpha to 0

        if testing:
            self.epsilon = 0.0
            self.alpha = 0
        else:
            self.trial_count += 1
            self.epsilon = 1 / (self.trial_count ** 2)

        return

    def build_state(self):

        ''' This may no longer be needed as the state is already passed into the  agent
        '''

        pass

        """ The build_state function is called when the agent requests data from the
            environment. The next waypoint, the intersection inputs, and the deadline
            are all features available to the agent. """

        # Collect data about the environment
        # waypoint = self.planner.next_waypoint() # The next waypoint
        # inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        # deadline = self.env.get_deadline(self)  # Remaining deadline

        ###########
        ## TO DO ##
        ###########
        # Set 'state' as a tuple of relevant data for the agent

        # state = (waypoint, inputs['light'], inputs['left'], inputs['oncoming'])
        #
        # return state

    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        ###########
        ## TO DO ##
        ###########
        # Calculate the maximum Q-value of all actions for a given state
        # print([v for v in self.Q[state].values()])
        maxQ = max(v for v in self.Q[state].values())

        return maxQ

    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        ###########
        ## TO DO ##
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0

        if self.learning:
            if state in self.Q.keys():
                pass
            else:
                self.Q[state] = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0}

        return

    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        self.createQ(state)                         # Create 'state' in Q-table
        # Set the agent state and default action
        # self.state = state
        # self.next_waypoint = self.planner.next_waypoint()
        # action = None

        ###########
        ## TO DO ##
        ###########
        # When not learning, choose a random action
        # When learning, choose a random action with 'epsilon' probability
        #   Otherwise, choose an action with the highest Q-value for the current state

        if self.learning:

            r = random.random()

            if random.random() < self.epsilon:

                while True:
                    action = random.choice(self.valid_actions)

                    if state[action] == None:
                        break

            else:
                # get the maximum Q value from all the actions available for the state
                max_val = self.get_maxQ(state)
                #print(max_val)

                # get the actions that have the highest values. Note that there may be more than one
                good_actions = [k for k,v in self.Q[state].items() if v == max_val]
                #print(good_actions)

                # randomly select from the actions available.
                while True:
                    action = random.choice(good_actions)

                    if state[action] == None:
                        break

        else:
            while True:
                action = random.choice(self.valid_actions)

                if state[action] == None:
                    break

        self.last_state = state
        self.last_action = action

        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives an award. This function does not consider future rewards
            when conducting learning. """

        ###########
        ## TO DO ##
        ###########
        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')

        if self.learning:
            currentQ = self.Q[state][action]
            self.Q[state][action] = reward * self.alpha + currentQ * (1 - self.alpha)

        return

    def set_reward(self, action, winner):

        reward = 0

        if winner == 0:
            reward = 1
        elif winner == 1:
            reward = -2
        else:
            reward = 5

        return reward

    def update(self, state, action, winner):

        """ The update function is called when a time step is completed in the environment for a given trial.

            This function will receive a reward, and learn if enabled.

            If the state is note provided, it will compute the reward based on the last state and action
        """

        if state:
            reward = self.set_reward(action, winner)
            self.learn(state, action, reward)
        else:
            reward = self.set_reward(self.last_action, winner)
            self.learn(self.last_state, self.last_action, reward)

        return action

    def get_state(self):
        return self.state

    def get_queues(self):
        return self.Q
